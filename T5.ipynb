{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:02:20.098477Z",
     "iopub.status.busy": "2021-09-05T13:02:20.098164Z",
     "iopub.status.idle": "2021-09-05T13:02:39.993189Z",
     "shell.execute_reply": "2021-09-05T13:02:39.992243Z",
     "shell.execute_reply.started": "2021-09-05T13:02:20.098448Z"
    },
    "id": "Ng-YsThhw5gI",
    "outputId": "396815c8-0382-48b9-a479-00b2d6e9bcd1"
   },
   "outputs": [],
   "source": [
    "# Installations\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:02:39.996915Z",
     "iopub.status.busy": "2021-09-05T13:02:39.996616Z",
     "iopub.status.idle": "2021-09-05T13:02:47.07667Z",
     "shell.execute_reply": "2021-09-05T13:02:47.075686Z",
     "shell.execute_reply.started": "2021-09-05T13:02:39.996885Z"
    },
    "id": "fqXufHSy1EOG"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import csv\n",
    "from csv import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:36:32.060685Z",
     "iopub.status.busy": "2021-09-01T18:36:32.060349Z",
     "iopub.status.idle": "2021-09-01T18:36:33.05847Z",
     "shell.execute_reply": "2021-09-01T18:36:33.057652Z",
     "shell.execute_reply.started": "2021-09-01T18:36:32.060652Z"
    },
    "id": "WU7V5zF61I5e",
    "outputId": "3ad28bae-49fa-47d8-cba0-649881367694"
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset_file = 'dataset.csv'\n",
    "dataset = load_dataset('csv', data_files=dataset_file, split='train')\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:36:38.315758Z",
     "iopub.status.busy": "2021-09-01T18:36:38.315422Z",
     "iopub.status.idle": "2021-09-01T18:36:52.091673Z",
     "shell.execute_reply": "2021-09-01T18:36:52.090881Z",
     "shell.execute_reply.started": "2021-09-01T18:36:38.315727Z"
    },
    "id": "yDk_UZTC1gLo",
    "outputId": "afb73d3c-b292-4c7e-f7f7-3ac5356d4c95"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "def tokenize(batch):\n",
    "    tokenized_input = tokenizer(batch['source'], padding='max_length', truncation=True, max_length=900)\n",
    "    tokenized_label = tokenizer(batch['target'], padding='max_length', truncation=True, max_length=900)\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "    return tokenized_input\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
    "val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-01T18:37:18.581221Z",
     "iopub.status.busy": "2021-09-01T18:37:18.58071Z",
     "iopub.status.idle": "2021-09-01T19:06:34.156034Z",
     "shell.execute_reply": "2021-09-01T19:06:34.15504Z",
     "shell.execute_reply.started": "2021-09-01T18:37:18.58117Z"
    },
    "id": "3a-bRiv72H4_",
    "outputId": "a6cebe89-1a64-4006-9652-6296612795b7"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "output_dir = 'output'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    eval_accumulation_steps=1, \n",
    "    learning_rate=0.001,\n",
    "    evaluation_strategy='steps', \n",
    "    remove_unused_columns=True, \n",
    "    run_name='run_name', \n",
    "    logging_steps=1000, \n",
    "    eval_steps=1000,\n",
    "    adam_beta1=0.6,\n",
    "    adam_beta2=0.6,\n",
    "    adam_epsilon=1.3e-8 \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir + '/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVUwz2Ms3CDn",
    "outputId": "7fca607e-f672-48e6-f5a7-3c8bd5a5e0ba"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model_dir = 'output/model'\n",
    "output_dir = 'output'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred[0], axis=2)\n",
    "    accuracy = accuracy_score(y_true=labels[0], y_pred=pred[0])\n",
    "    recall = recall_score(y_true=labels[0], y_pred=pred[0], average='macro')\n",
    "    precision = precision_score(y_true=labels[0], y_pred=pred[0], average='macro')\n",
    "    f1 = f1_score(y_true=labels[0], y_pred=pred[0], average='macro')\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "pred_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_eval_batch_size=1,\n",
    "    remove_unused_columns=True,\n",
    "    eval_accumulation_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=pred_args, compute_metrics=compute_metrics)\n",
    "\n",
    "preds, labels, metrics = trainer.predict(val_dataset)\n",
    "preds_tokens = preds[0].argmax(axis=2)\n",
    "print(metrics)\n",
    "\n",
    "decoded_sources = []\n",
    "for row in val_dataset:\n",
    "    decoded_sources.append(tokenizer.decode(row['input_ids']))\n",
    "\n",
    "decoded_preds = [tokenizer.decode(pred) for pred in preds_tokens]\n",
    "decoded_labels = [tokenizer.decode(label) for label in labels]\n",
    "\n",
    "output = pd.DataFrame({'Source Text': decoded_sources, 'Target Text': decoded_labels, 'Generated Text': decoded_preds})\n",
    "output.to_excel(output_dir + \"/predictions.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T13:07:23.342169Z",
     "iopub.status.busy": "2021-09-05T13:07:23.341835Z",
     "iopub.status.idle": "2021-09-05T15:32:42.619025Z",
     "shell.execute_reply": "2021-09-05T15:32:42.618085Z",
     "shell.execute_reply.started": "2021-09-05T13:07:23.342138Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating with beam search\n",
    "model_dir = 'output/model'\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "dev_outputs = []\n",
    "with open('dev_dataset.csv', 'r') as read_obj:\n",
    "  csv_reader = reader(read_obj)\n",
    "  header = next(csv_reader)\n",
    "  if header != None:\n",
    "    for row in csv_reader:\n",
    "      source = row[0]\n",
    "      target = row[1]\n",
    "      input_ids = tokenizer(source, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=900).input_ids\n",
    "      output = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=1, max_length=900)\n",
    "      generated = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "      dev_outputs.append([source, target, generated[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-05T15:32:42.620968Z",
     "iopub.status.busy": "2021-09-05T15:32:42.620593Z",
     "iopub.status.idle": "2021-09-05T15:32:42.636865Z",
     "shell.execute_reply": "2021-09-05T15:32:42.636068Z",
     "shell.execute_reply.started": "2021-09-05T15:32:42.620929Z"
    }
   },
   "outputs": [],
   "source": [
    "header = ['source','target','generated']\n",
    "data = [[source, target, generated] for [source, target, generated] in dev_outputs]\n",
    "with open(r'dev_outputs.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
